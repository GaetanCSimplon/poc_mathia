import os
from dotenv import load_dotenv
from mistralai import Mistral
from logic.math_tools import calculate
from logic.adaptative import analyze_student_error
import json

load_dotenv()

api_key = os.getenv("MISTRAL_API_KEY")
if not api_key:
    raise ValueError("Cl√© API Mistral introuvable.")

client = Mistral(api_key=api_key)

tools = [
    {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "Utilise cet outil pour effectuer TOUS les calculs math√©matiques.",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {"type": "string", "description": "L'expression math√©matique (ex: '12 * 5')"},
                },
                "required": ["expression"],
            },
        },
    }
]

# is_voice_mode
def get_ai_response(user_message: str, conversation_history: list = None, is_voice_mode: bool = False) -> str:
    if conversation_history is None:
        conversation_history = []
    
    diagnostic = analyze_student_error(user_message, "Inconnu", conversation_history)
    
    adaptative_prompt_section = ""
    if diagnostic:
        print(f"[DEBUG] Diagnostic : {diagnostic['name']}")
        adaptative_prompt_section = f"""
        DIAGNOSTIC PEDAGOGIQUE (PRIORITAIRE) :
        L'√©l√®ve semble commettre l'erreur type : {diagnostic['name']} ({diagnostic['category']}).
        
        STRATEGIE DE REMEDIATION A APPLIQUER :
        {diagnostic['remediation']}
        
        Suis scrupuleusement cette strat√©gie pour ta r√©ponse.
        """


    # Consignes de base
    base_prompt = """
    TU ES UN PROFESSEUR DE MATHS P√âDAGOGUE, PAS UNE CALCULATRICE.
    
    R√àGLES ABSOLUES DE COMPORTEMENT (CRITIQUE) :
            Tu es MathIA, un assistant p√©dagogique virtuel pour des √©l√®ves de cycle 2 (CP, CE1, CE2) et cycle 3 (CM1, CM2).
            Tes objectifs : 
            1. Aider l'√©l√®ve √† r√©soudre des exercices de math√©matiques.
            2. NE JAMAIS donner la r√©ponse directement. C'est une r√®gle absolue.
            3. Si l'√©l√®ve pose une question, guide-le avec une question plus simple ou une m√©thode (ex: compter sur les doigts, visualiser des objets).
            4. Sois tr√®s encourageant, utilises des √©mojis, et parle avec des phrases courtes et simples.
            5. Si l'√©l√®ve se trompe, ne dis pas juste "non", explique pourquoi ou propose une autre approche.
            6. Si un calcul est n√©cessaire, utilise TOUJOURS l'outil 'calculate'.
            7. Sois encourageant et clair.
    """

    # ADAPTATION DU PROMPT SELON LE MODE
    if is_voice_mode:
        format_instructions = """
        CONSIGNES SP√âCIFIQUES AUDIO :
        1. Tu parles √† l'oral. Tes r√©ponses doivent √™tre courtes et percutantes.
        2. INTERDICTION D'UTILISER DU LATEX OU DU MARKDOWN (pas de $$, pas de **, pas de #).
        3. √âcris les maths pour qu'elles soient lues naturellement (ex: dis "3 fois 5" et non "3 * 5").
        4. Ne fais pas de listes √† puces, fais des phrases compl√®tes.
        5. INTERDICTION D'UTILISER LES EMOJIS DANS TES REPONSES (n'√©nonce pas les √©mojis que tu utiliserais dans tes r√©ponses textuelles).
        6. Sois chaleureux et direct.
        """
    else:
        format_instructions = """
        CONSIGNES DE FORMATAGE (MODE TEXTE/CHAT) :
        - Pour les formules math√©matiques, utilise TOUJOURS le format LaTeX encadr√© par des dollars (ex: $x^2$).
        - Utilise $$ formule $$ pour centrer les √©quations importantes.
        - Utilise des sauts de ligne, des √©mojis (üéì, ‚ú®) et des listes √† puces pour a√©rer le texte.

        """

    system_prompt = {
        "role": "system",
        "content": base_prompt + adaptative_prompt_section + format_instructions
    }

    messages = [system_prompt] + conversation_history + [{"role": "user", "content": user_message}]

    # Appel LLM
    response = client.chat.complete(
        model="mistral-small-latest",
        messages=messages,
        tools=tools,
        tool_choice="auto"
    )

    assistant_message = response.choices[0].message
    tool_calls = assistant_message.tool_calls

    # Gestion des outils
    if tool_calls:
        messages.append(assistant_message)
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            
            if function_name == "calculate":
                expression = function_args.get("expression")
                print(f"[DEBUG] Calcul : {expression}")
                result = calculate(expression)
                
                messages.append({
                    "role": "tool",
                    "name": function_name,
                    "content": str(result),
                    "tool_call_id": tool_call.id
                })

        final_response = client.chat.complete(
            model="mistral-small-latest",
            messages=messages
        )
        return final_response.choices[0].message.content

    else:
        return assistant_message.content