# Fichier: logic/llm_engine.py
import os
from dotenv import load_dotenv
from mistralai import Mistral
from math_tools import calculate
import json

load_dotenv()

# R√©cup√©ration de la cl√© API
api_key = os.getenv("MISTRAL_API_KEY")
if not api_key:
    raise ValueError("Cl√© API Mistral introuvable. V√©rifiez votre fichier .env")

# Initialisation du client Mistral 
client = Mistral(api_key=api_key)

# 1. D√©finition des outils (Tools)

tools = [
    {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "Utilise cet outil pour effectuer TOUS les calculs math√©matiques. Ne calcule jamais de t√™te.",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "L'expression math√©matique √† calculer (ex: '12 * 5')",
                    }
                },
                "required": ["expression"],
            },
        },
    }
]

def get_ai_response(user_message: str, conversation_history: list = None) -> str:
    """
    Fonction principale qui g√®re le dialogue et l'appel d'outil.
    """
    if conversation_history is None:
        conversation_history = []

    # D√©finition du "System Prompt" (La personnalit√© du prof)
    system_prompt = {
        "role": "system",
        "content": ("""
            Tu es MathIA, un assistant p√©dagogique virtuel pour des √©l√®ves de cycle 2 (CP, CE1, CE2) et cycle 3 (CM1, CM2).
            Tes objectifs : 
            1. Aider l'√©l√®ve √† r√©soudre des exercices de math√©matiques.
            2. NE JAMAIS donner la r√©ponse directement. C'est une r√®gle absolue.
            3. Si l'√©l√®ve pose une question, guide-le avec une question plus simple ou une m√©thode (ex: compter sur les doigts, visualiser des objets).
            4. Sois tr√®s encourageant, utilises des √©mojis, et parle avec des phrases courtes et simples.
            5. Si l'√©l√®ve se trompe, ne dis pas juste "non", explique pourquoi ou propose une autre approche.
            6. Si un calcul est n√©cessaire, utilise TOUJOURS l'outil 'calculate'.
            7. Sois encourageant et clair.
            """
        )
    }

    # Pr√©paration des messages pour l'API
    messages = [system_prompt] + conversation_history + [{"role": "user", "content": user_message}]

    # 1er Appel au LLM : "Analyse la demande"
    response = client.chat.complete(
        model="mistral-small", # Ou "open-mistral-nemo" (moins cher)
        messages=messages,
        tools=tools,
        tool_choice="auto" # Le LLM d√©cide s'il a besoin de l'outil
    )

    # R√©cup√©ration de la r√©ponse
    assistant_message = response.choices[0].message
    tool_calls = assistant_message.tool_calls

    # CAS A : Le LLM veut utiliser l'outil (la calculatrice)
    if tool_calls:
        # On ajoute la demande de l'assistant √† l'historique (pour qu'il s'en souvienne)
        messages.append(assistant_message)

        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            
            if function_name == "calculate":
                expression = function_args.get("expression")
                print(f"[DEBUG] Le LLM demande le calcul : {expression}")
                
                # On ex√©cute la fonction Python (Notre fichier math_tools.py)
                result = calculate(expression)
                print(f"[DEBUG] R√©sultat Python : {result}")

                # On renvoie le r√©sultat au LLM comme un message "tool"
                messages.append({
                    "role": "tool",
                    "name": function_name,
                    "content": str(result),
                    "tool_call_id": tool_call.id
                })

        # 2√®me Appel au LLM : "Maintenant que tu as le r√©sultat, r√©ponds √† l'√©l√®ve"
        final_response = client.chat.complete(
            model="mistral-large-latest",
            messages=messages
        )
        return final_response.choices[0].message.content

    # CAS B : Pas de calcul n√©cessaire (ex: "Bonjour")
    else:
        return assistant_message.content

if __name__ == "__main__":
    print("--- üéì MathIA Console (Tape 'exit' pour quitter) ---")
    
    # 1. Initialisation de la m√©moire vide
    history = []
    
    while True:
        # 2. On attend que l'√©l√®ve √©crive quelque chose
        user_input = input("\nToi üßë‚Äçüéì : ")
        
        # Condition de sortie
        if user_input.lower() in ["exit", "quit", "quitter"]:
            print("MathIA üëã : √Ä bient√¥t !")
            break
            
        # 3. On appelle le cerveau (en lui donnant l'historique actuel)
        # Note : La fonction get_ai_response va combiner System + History + Question actuelle
        reponse_ia = get_ai_response(user_input, conversation_history=history)
        
        print(f"MathIA ü§ñ : {reponse_ia}")
        
        # 4. CRUCIAL : Mise √† jour de la m√©moire pour le prochain tour
        # On ajoute ce que l'√©l√®ve vient de dire
        history.append({"role": "user", "content": user_input})
        # On ajoute ce que l'IA vient de r√©pondre
        history.append({"role": "assistant", "content": reponse_ia})